---
title: "Untitled"
author: "Jonathan Bourne"
date: "20 January 2016"
output: html_document
---

Setup packages and install packages
```{r}
packages <- c("randomForest","dplyr", "ggplot2", "caret", "Amelia", "tidyr","pROC") #, "betareg", "mi", "Amelia", "Zelig", ) #If amelia is used Betadist and mi can be removed

sapply(packages, library, character.only = TRUE)
select <- dplyr::select #Clash between dplyr and MASS which is loaded by Caret
setwd("~/Dropbox/MajoR")
#basewd <- "C:/Users/Jonno/Dropbox/MajoR"
#setwd(basewd)

load("data.Rdata")
#data.raw<- read.csv("train.csv")
#test <- read.csv("test_2.csv")
#data <- data.raw

```


Time series function
```{r}

timestats <- function(timeseries){
  #Takes a time series without Na values and extracts Mean,median, max,min var, geomettric mean, and returns a data frame. N.B the time series doesn't actually have to be in a time series format.
gm_mean <- function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
} #http://stackoverflow.com/questions/2602583/geometric-mean-is-there-a-built-in

x <- cbind(colMeans(timeseries),
           sapply(timeseries, var),
           sapply(timeseries, median),
           sapply(timeseries, min), 
           sapply(timeseries, max),
           sapply(timeseries+1, gm_mean)-1
           ) %>% data.frame

names(x) <- c("means", "vars","median","mins", "max", "gm_mean" )
x
}
```

Find columns to remove
```{r}

lowvar <-nearZeroVar(data[1:28]) #these are the same so can be removed from the datasets column 17 is constant

NASdata <- sapply( data[,1:28], function(n) sum(is.na(n)))

highNAS <-which(NASdata/nrow(data) >0.2) #returns index of major missing elements

```


Imputation code chunk, takes the relvant variables, and first imputs the time series, creates a summary statistics of the time series. After recombining the imputed data set an new variables, the features are then imputed. the resulting number of imputations is timeimput*featureimput

```{r}
timeimput <-5
featureimput <- 5
number_cols <- 119
#removes the junk variables
data2 <- data[1:200,-c(lowvar, highNAS,29:211)] %>% mutate(Id = paste("X",Id, sep=""))

#reformat for time series imputations
datatime <- data[1:200,] %>% select(Ret_2:Ret_120) %>% t %>% data.frame %>% 
  mutate(timeID = 1:119) %>% gather(., key = Id, value = ROI, -timeID ) 

#imputation includes interaction within crossectional timeseries to try and reduce variance
unique(datatime$Id)
group_size <- number_cols*10
number_groups <- nrow(datatime)/group_size

name_series <- c(0, seq(from = group_size, to = nrow(datatime), by = group_size ))

imputs3 <- vector("list", length(name_series)-1)

#for large data sets even a high power computer suffers, to this end I have broken up the imutation into smaller chunks, the imputed chunks are then reconstructed later for continued analysis and imputation.
set.seed(1928)
for (n in 2:length(name_series)) {
  ids <-(name_series[n-1]+1):(name_series[n])
  imputs3[[n-1]] <- datatime[ids,] %>% amelia(., ts="timeID", 
                                              cs = "Id", 
                                      polytime = 2, intercs = TRUE, 
                                      m= timeimput,
                                      empri = group_size*0.05)

  }


#tscsPlot(imputs3, cs ="X26", var = "ROI")

#takes the imputations from each selection of stock prices and reconstructs them into a continuous single imputation
reconstr <- matrix(NA, nrow = nrow(data2), ncol =3) %>% data.frame
names(reconstr) <- names(imputs3[[1]]$imputations[[1]])
reconstrList <- replicate(timeimput, reconstr, simplify=FALSE)

y = rep(NA,length(name_series)-1)
z = rep(NA,length(name_series)-1)

#Iterates through each selection of variables and each imputation
for (i in 1:timeimput) { 

    x <- matrix(NA, nrow = nrow(data2), ncol =3) %>% data.frame
  
    for (n in 1:(length(name_series)-1)){

    rowids <- ((n-1)*group_size+1):(n*group_size)
    x[rowids,] <- imputs3[[n]]$imputations[[i]]
    #y[n] <-x %>% is.na %>% sum
    #z[n-1] <- ((n-1)*group_size+1)
    }
  reconstrList[[i]][1:nrow(data),] <-x
}


#recombines the data with the feature data and the timeseries descriptive statistics
PreFeatureImp <- vector("list", timeimput)


for (i in 1:timeimput)  {
    x <- reconstrList[[i]] %>% spread(., key = Id, value = ROI) %>% select(-timeID)
    timestat <- timestats(x) 
    x <- x %>%  t %>% data.frame %>% bind_cols(data2,timestat,.)
    names(x)[31:149]<- names(data)[29:147]
    PreFeatureImp[[i]] <- x
}


reconstr <- matrix(NA, nrow = nrow(data2), ncol =number_cols) %>% data.frame
names(reconstr) <- names(imputs3[[1]]$imputations[[1]])
reconstrList <- replicate(timeimput, reconstr, simplify=FALSE)


set.seed(1928)
for (n in 2:length(name_series)) {
  #identify time series that will be selected
  ids <-(name_series[n-1]+1):(name_series[n])
  #imput select time series m times
  a.out <- datatime[ids,] %>% amelia(., ts="timeID", 
                                              cs = "Id", 
                                      polytime = 2, intercs = TRUE, 
                                      m= timeimput,
                                      empri = group_size*0.05)
  
  #identify row numbers in final dataframe and insert the data into the rows
  rowids <- (name_series[n-1]/number_cols+1):(name_series[n]/number_cols)
  for ( i in 1:timeimput) {
    reconstrList[[i]][rowids,] <- a.out$imputations[[i]] %>% 
      spread(., key = Id, value = ROI) %>% select(-timeID) %>% t
    }
  }


#Imputs the missing Features based on the Features and also time series stats
ameliaList <- vector("list",length(PreFeatureImp))

for (i in 1:length(ameliaList))  {
  ameliaList <- PreFeatureImp[[i]] %>% select(Feature_3:gm_mean) %>% amelia(m= featureimput)

}


```


not usable on large data sets
```{r}

# for (i in 1:timeimput)  {
#     x <- imputs3$imputations[[i]] %>% spread(., key = Id, value = ROI) %>% select(-timeID)
#     timestat <- timestats(x) 
#     x <- x %>%  t %>% data.frame %>% bind_cols(data2,timestat,.)
#     names(x)[31:149]<- names(data)[29:147]
#     PreFeatureImp[[i]] <- x
# }

```


create data frame for classification
```{r}
data3 <- na.roughfix(data[,-c(2,3,5,11,21,29:211, 17)])
```


create plus one bounds
```{r}
standarddevsPlusOne <- sd(data$Ret_PlusOne)*3
meanPlusOne <- mean(data$Ret_PlusOne)
lowerboundone <- meanPlusOne-standarddevsPlusOne
upperboundone <- meanPlusOne+standarddevsPlusOne

PlusOnecat <- rep(0, 40000)
PlusOnecat[data$Ret_PlusOne <lowerboundone] <- 1
PlusOnecat[data$Ret_PlusOne >upperboundone] <- 1



PlusOnecatEx <- rep(0, 40000)
PlusOnecatEx[data$Ret_PlusOne <lowerboundone] <- -1
PlusOnecatEx[data$Ret_PlusOne >upperboundone] <- 1


PosNegcat <- rep(0, 40000)
PosNegcat[data$Ret_PlusOne >0] <- 1
PosNegcat <- as.factor(PosNegcat)


PosNegcat2 <- rep(0, 40000)
PosNegcat2[data$Ret_PlusTwo >0] <- 1
PosNegcat2 <- as.factor(PosNegcat2)

```


create plus Two bounds
```{r}
standarddevsPlusTwo <- stdev(data$Ret_PlusTwo)*3
meanPlusTwo <- mean(data$Ret_PlusTwo)
lowerboundTwo <- meanPlusTwo-standarddevsPlusTwo
upperboundTwo <- meanPlusTwo+standarddevsPlusTwo

PlusTwocat <- rep(0, 40000)
PlusTwocat[data$Ret_PlusTwo <lowerboundTwo] <- -1
PlusTwocat[data$Ret_PlusTwo >upperboundTwo] <- 1
```

Calculate medians
```{r}

catIds <- c(-1,0,1)
x <- matrix(NA, nrow = 2, ncol = 3)

x[1,] <-sapply(1:3, function(n) {
    data$Ret_PlusOne[PlusOnecat == catIds[n]] %>% median
        }
)

x[2,] <-sapply(1:3, function(n) {
data$Ret_PlusOne[PlusOnecat == catIds[n]] %>% mean   
  }
)

```


Classify extreme values
#it looks like Knn is not a good classifier rf is now working but is very slow
```{r}

train_control <- trainControl(method="cv", number=5, verboseIter = TRUE)

class1 <- train(x = data3,y = as.factor(PlusOnecat),  
                preProcess = c("center", "scale"),
                trControl = train_control, 
                tuneLength = 10, method = 'knn' )

preds <- predict(class1, data3)

conf <- confusionMatrix(preds, as.factor(PlusOnecat))

train_control <- trainControl(method="cv", number=10, verboseIter = TRUE)

```


Predict Positive negative day +1
```{r}
classPosNeg <- train(x = data3,y = PosNegcat,  
                    method = "treebag",
                    nbagg = 50,
                    metric = "ROC",
                    trControl = ctrl)

predsPosNeg <- predict(classPosNeg, data3)
confPosNeg <- confusionMatrix(predsPosNeg, PosNegcat)

confPosNegExtreme <- confusionMatrix(predsPosNeg[as.logical(PlusOnecatEx)], PosNegcat[as.logical(PlusOnecatEx)])

medianpos <- median(data$Ret_PlusOne[as.logical(as.numeric(as.character(PosNegcat)))])
medianneg <- median(data$Ret_PlusOne[-as.logical(as.numeric(as.character(PosNegcat)))])

```


Predict Positive negative day +2
```{r}
classPosNeg2 <- train(x = data3,y = PosNegcat2,  
                    method = "treebag",
                    nbagg = 50,
                    metric = "ROC",
                    trControl = ctrl)

predsPosNeg2 <- predict(classPosNeg2, data3)
confPosNeg2 <- confusionMatrix(predsPosNeg2, PosNegcat2)

medianpos2 <- median(data$Ret_PlusTwo[as.logical(as.numeric(as.character(PosNegcat2)))])
medianneg2 <- median(data$Ret_PlusTwo[-as.logical(as.numeric(as.character(PosNegcat2)))])

```



```{r}
imputs1 <- data[,-c(2,3,5,11,21,29:211, 17)] %>% amelia(., idvars = "Id", m = 80)


train_control <- trainControl(method="cv", number=10)
mod1 <- train(x = data3[, -c(21:22)], y = data$Ret_PlusOne, trControl = train_control, method = "rf")

mod1 <- train(x = imputs1$imputations, y = data$Ret_PlusOne, trControl = train_control, method = "lm")

modz <- zelig(x = imputs1, y = data$Ret_PlusOne,  model="ls" )

zelig(gdp_pc ~ trade + civlib, model="ls", data=a.out)

```


Multilinear model comparison
```{r}


train_control <- trainControl(method="repeatedcv", number=10, repeats = 5, verboseIter = TRUE)

set.seed(1928)
mod1 <- train(x = data3, y = data$Ret_PlusOne, method = "lm")

set.seed(1928)
mod2 <- train(x = imputs1$imputations[[1]], y = data$Ret_PlusOne, method = "lm")

results <- resamples(list(mod1=mod1, mod2=mod2))


#training models on an imputed list
mod3 <- vector("list", 5)

for ( i in 1:5){
set.seed(1928)
mod3[[i]] <- train(x = imputs1$imputations[[i]], y = data$Ret_PlusOne, method = "lm")
}

results <- resamples(list(v1=mod3[[1]], v2=mod3[[2]],v3=mod3[[3]],v4=mod3[[4]],v5=mod3[[5]]))

models <- paste("mod3[[", 1:5, "]]", sep = "")

summod <- sapply(models, function(n) {
        predict(eval(parse(text=n)))  
}) 

meanEns <- rowMeans(summod)

  comparison <-   sapply(models, function(n) {
     postResample(predict(eval(parse(text=n)), data), data$Ret_PlusOne)
   }
 ) %>% t %>% data.frame

#combines the single models and trhe full one together
comparison <-  postResample(rowMeans(summod), data$Ret_PlusOne)%>% rbind(comparison)
  
# compares multiple models 
models <- c("mod1", "mod2")

comparison <- sapply(models, function(n) {
     postResample(predict(eval(parse(text=n)), data), data$Ret_PlusOne)
   }
 ) %>% t %>% data.frame

```


Dealing with time series
```{r}
timedat <- data %>% select(Ret_2:Ret_160)
imputstime <- amelia(timedat[,1:5], ts = time)
testts <- ts(testzoo)

timedat3 <- timedat %>% t %>% zoo
#percentage missing data, there isnt a lot
sum(is.na(timedat3))/prod(dim(timedat3))

timedat2 <- timedat %>% t
testzoo <- zoo(timedat2) 

#number os missing data points
colSums(is.na(testzoo))


#simple non imputed result
testzoointerp <- na.approx(testzoo)



#number of missing values is now zero
colSums(is.na(testzoointerp))

tsinterpd <- ts(testzoointerp[,3])
tidmod <- ets(tsinterpd)
tidmod <- HoltWinters(tsinterpd, beta=FALSE, gamma=FALSE)
accuracy(tidmod)
forecast(tidmod, 60)

names(forecast(tidmod, 60))

plot(forecast(tidmod, 60))
```

